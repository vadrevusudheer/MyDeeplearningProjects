# Step 1: Installation and Setup

# Installing TensorFlow
! pip install -q tensorflow-gpu

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 2: Importing the dataset from kaggle to google colab

# install kaggle API
! pip install kaggle

# create a directory as kaggle
! mkdir -p ~/.kaggle

# import kaggle API
from google.colab import files
uploaded = files.upload()

# copy API key to kaggle directory
! cp kaggle.json ~/.kaggle

# disable the API key
! chmod 600 /root/.kaggle/kaggle.json

# list of datasets
! kaggle datasets list

# import the dataset
! kaggle datasets download -d mlg-ulb/creditcardfraud

# unzipping dataset
! unzip /content/creditcardfraud.zip

dataset_1  = pd.read_csv('/content/creditcard.csv')

dataset_1.head()

# Step 3: Data Preprocessing

dataset_1.shape

# checking the null values
dataset_1.isnull().sum()

dataset_1.info()

# observations in each class
dataset_1['Class'].value_counts()

# balence the dataset
fraud = dataset_1[dataset_1['Class']==1]
non_fraud = dataset_1[dataset_1['Class']==0]

fraud.shape, non_fraud.shape

# random selection of samples
non_fraud_t = non_fraud.sample(n=492)

non_fraud_t.shape

# merge dataset
dataset = fraud.append(non_fraud_t, ignore_index=True)

print(dataset)

# observations in each class
dataset['Class'].value_counts()

# matrix of features
x = dataset.drop(labels=['Class'], axis=1)

# dependent variable
y = dataset['Class']

x.shape, y.shape

# splitting the dataset into train and test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

x_train.shape, x_test.shape

# feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

x_train.shape, x_test.shape

# reshape the dataset
x_train = x_train.reshape(787, 30, 1)
x_test = x_test.reshape(197, 30, 1)

x_train.shape, x_test.shape

# Step 4: Building the model

# defining an object
model = tf.keras.models.Sequential()

# first CNN layer
model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=2, padding='same', activation='relu', input_shape = (30, 1)))

# batch normalization
model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
model.add(tf.keras.layers.Dropout(0.2))

# second CNN layer
model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=2, padding='same', activation='relu'))

# batch normalization
model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
model.add(tf.keras.layers.Dropout(0.3))

# flatten layer
model.add(tf.keras.layers.Flatten())

# first dense layer
model.add(tf.keras.layers.Dense(units=64, activation='relu'))

# dropout layer
model.add(tf.keras.layers.Dropout(0.3))

# output layer
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

model.summary()

opt = tf.keras.optimizers.Adam(learning_rate=0.0001)

model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

# Step 5: Training the model

history = model.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test))

# model predictions
y_pred = model.predict_classes(x_test)

print(y_pred[12]), print(y_test[12])

# confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)

acc_cm = accuracy_score(y_test, y_pred)
print(acc_cm)

# Step 6: Learning Curve

def learning_curve(history, epoch):

  # training vs validation accuracy
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

  # training vs validation loss
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

learning_curve(history, 25)
