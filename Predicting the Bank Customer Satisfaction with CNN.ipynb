# Step 1: Installation and Setup

# Installing TensorFlow
! pip install -q tensorflow-gpu

import tensorflow as tf
print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Step 2: Importing dataset from kaggle to google colab

# installing kaggle API
! pip install -q kaggle

# creating a directory as kaggle
! mkdir -p ~/.kaggle

# importing kaggle API
from google.colab import files
uploaded = files.upload()

# copy API key to kaggle directory
! cp kaggle.json ~/.kaggle/

# disable API key
! chmod 600 /root/.kaggle/kaggle.json

# list of the datasets
! kaggle datasets list

# importing dataset from kaggle
! kaggle competitions download -c santander-customer-satisfaction

# unzip datasets
! unzip /content/train.csv.zip

dataset = pd.read_csv('/content/train.csv')

dataset.head()

# Step 3: Data Preprocessing

dataset.shape

# independent variables (Matrix of features)
x = dataset.drop(labels=['ID','TARGET'], axis=1)

# dependent variable
y = dataset['TARGET']

x.shape, y.shape

# splitting the dataset into train and test set
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)

x_train.shape, x_test.shape

# Step 4: Remove constant, Quasi constant and duplicate features

from sklearn.feature_selection import VarianceThreshold

rm_f = VarianceThreshold(threshold=0.01)
x_train = rm_f.fit_transform(x_train)
x_test = rm_f.transform(x_test)

x_train.shape, x_test.shape

369-266

# remove duplicate features
x_train_t = x_train.T
x_test_t = x_test.T

x_train_t = pd.DataFrame(x_train_t)
x_test_t = pd.DataFrame(x_test_t)

x_train_t.shape, x_test_t.shape

x_train_t.duplicated()

# number of duplicate features
x_train_t.duplicated().sum()

duplicated_features = x_train_t.duplicated()
print(duplicated_features)

features_to_keep = [not index for index in duplicated_features]
print(features_to_keep)

x_train = x_train_t[features_to_keep].T
x_test = x_test_t[features_to_keep].T

x_train.shape, x_test.shape

266-250

# feature scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_train.shape, x_test.shape

# reshape the dataset
x_train = x_train.reshape(60816, 250, 1)
x_test = x_test.reshape(15204, 250, 1)

x_train.shape, x_test.shape

y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

# Step 5: Building the model

# define an object
model = tf.keras.models.Sequential()

# first CNN layer
model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape = (250, 1)))

# batch normalization
model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
model.add(tf.keras.layers.Dropout(0.3))

# second CNN layer
model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))

# batch normalization
model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
model.add(tf.keras.layers.Dropout(0.5))

# third CNN layer
model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'))

# batch normalization
model.add(tf.keras.layers.BatchNormalization())

# maxpool layer
model.add(tf.keras.layers.MaxPool1D(pool_size=2))

# dropout layer
model.add(tf.keras.layers.Dropout(0.5))

# flatten layer
model.add(tf.keras.layers.Flatten())

# first dense layer (fully connected layer)
model.add(tf.keras.layers.Dense(units=256, activation='relu'))

# dropout layer
model.add(tf.keras.layers.Dropout(0.5))

# output layer
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

model.summary()

opt = tf.keras.optimizers.Adam(learning_rate=0.00005)

# compile the model
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

# Step 6: Training the model

history =  model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

# model predictions
y_pred = model.predict_classes(x_test)

print(y_pred[12]), print(y_test[12])

# confusion matrix
from sklearn.metrics import  confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)

acc_cm = accuracy_score(y_test, y_pred)

print(acc_cm)

# Step 7: Learning Curve

def learning_curve(history, epoch):

  # training vs validation accuracy
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

  # training vs validation loss
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

learning_curve(history, 10)
